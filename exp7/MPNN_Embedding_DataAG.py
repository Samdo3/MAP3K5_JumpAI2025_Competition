# ## 0. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

# %%
import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor
import warnings
warnings.filterwarnings('ignore')

# RDKit
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski, rdMolDescriptors, Crippen, QED

# Chemprop
from chemprop import data, models, nn as chemprop_nn
from chemprop import featurizers
from chemprop.nn.metrics import ChempropMetric
from lightning import pytorch as pl
from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint
from lightning.pytorch import seed_everything
from lightning.pytorch.loggers import TensorBoardLogger



# CheMeleon ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (í•„ìš”ì‹œ)
from urllib.request import urlretrieve

# ML ë¼ì´ë¸ŒëŸ¬ë¦¬
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from catboost import CatBoostRegressor, Pool

# ì‹œê°í™”
import matplotlib.pyplot as plt
import seaborn as sns

# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

seed_everything(SEED, workers=True)

# ## 1. Phase 1: ë°ì´í„° ì¤€ë¹„ ë° íŠ¹ì§• ê³µí•™

# %%
# ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv('../data/train_dataset_with_3source.csv')
test_df = pd.read_csv('../data/test.csv')

test_df = test_df.rename(columns={'Smiles': 'SMILES'}) # train, test ì»¬ëŸ¼ëª… ë¶ˆì¼ì¹˜

print(f"Train ë°ì´í„°: {train_df.shape}")
print(f"Test ë°ì´í„°: {test_df.shape}")
print(f"\nTrain pIC50 ë¶„í¬:\n{train_df['pIC50'].describe()}")

def enumerate_smiles(smiles, num_versions=10):
    """í•˜ë‚˜ì˜ SMILESì— ëŒ€í•´ ì—¬ëŸ¬ ë²„ì „ì˜ SMILESë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return [smiles]
    
    # doRandom=True ì˜µì…˜ì„ ì‚¬ìš©í•´ ë¬´ì‘ìœ„ SMILES ìƒì„±
    return [Chem.MolToSmiles(mol, doRandom=True) for _ in range(num_versions)]

# ğŸ”¥ 1. ì¤‘ìš”: ì¦ê°• ì „ì— ê³ ìœ  GroupID í• ë‹¹
# ê° ê³ ìœ í•œ ì›ë³¸ ë¶„ìì— IDë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
train_df['GroupID'] = range(len(train_df))

# --- Phase 1 ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„ ë§ˆì§€ë§‰ì— ì¶”ê°€ ---
print("ì´ˆê³ í™œì„± ë¶„ì ë°ì´í„° ì¦ê°• (SMILES Enumeration)...")
high_activity_df = train_df[train_df['pIC50'] > 10].copy()
augmented_rows = []

NUM_VERSIONS = 3 # í˜„ì¬ ì„¤ì • ìœ ì§€

for _, row in high_activity_df.iterrows():
    original_smiles = row['SMILES']
    new_smiles_list = enumerate_smiles(original_smiles, num_versions=NUM_VERSIONS)
    
    for new_smiles in new_smiles_list:
        new_row = row.copy() # ğŸ”¥ row.copy()ë¥¼ í†µí•´ GroupIDê°€ ìë™ìœ¼ë¡œ ìƒì†ë©ë‹ˆë‹¤.
        new_row['SMILES'] = new_smiles
        augmented_rows.append(new_row)

# ì›ë³¸ ë°ì´í„°ì™€ ì¦ê°•ëœ ë°ì´í„°ë¥¼ í•©ì¹¨
augmented_df = pd.DataFrame(augmented_rows)
train_df = pd.concat([train_df, augmented_df], ignore_index=True)

print(f"ë°ì´í„° ì¦ê°• í›„ Train ë°ì´í„°: {train_df.shape}")
# ì´í›„ ëª¨ë“  í•™ìŠµì€ train_df_augmentedë¥¼ ì‚¬ìš©

# RDKit ë¶„ì íŠ¹ì§• ê³„ì‚°
def calculate_molecular_features(smiles):
    """ë¶„ì êµ¬ì¡°ë¡œë¶€í„° í…Œì´ë¸” íŠ¹ì§• ì¶”ì¶œ"""
    features = {}
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None
        
        # ê¸°ë³¸ ë¶„ì íŠ¹ì„±
        features['MolWt'] = Descriptors.MolWt(mol)
        features['LogP'] = Crippen.MolLogP(mol)
        features['NumHDonors'] = Lipinski.NumHDonors(mol)
        features['NumHAcceptors'] = Lipinski.NumHAcceptors(mol)
        features['NumRotatableBonds'] = Lipinski.NumRotatableBonds(mol)
        features['NumHeavyAtoms'] = Lipinski.HeavyAtomCount(mol)
        features['NumAromaticRings'] = Lipinski.NumAromaticRings(mol)
        features['TPSA'] = rdMolDescriptors.CalcTPSA(mol)
        features['QED'] = QED.qed(mol)
        
        # ì¶”ê°€ íŠ¹ì§•
        features['NumHeteroatoms'] = Lipinski.NumHeteroatoms(mol)
        features['RingCount'] = Lipinski.RingCount(mol)
        features['FractionCsp3'] = rdMolDescriptors.CalcFractionCSP3(mol)
        features['NumAliphaticRings'] = Lipinski.NumAliphaticRings(mol)
        features['NumSaturatedRings'] = Lipinski.NumSaturatedRings(mol)
               
        return features
    except Exception as e:
        print(f"Error processing SMILES '{smiles}': {e}") # ë””ë²„ê¹… ì‹œ ì‚¬ìš©
        return None

# Train ë°ì´í„° íŠ¹ì§• ê³„ì‚°
print("ë¶„ì íŠ¹ì§• ê³„ì‚° ì¤‘...")
train_features = []
for smiles in train_df['SMILES']:
    features = calculate_molecular_features(smiles)
    train_features.append(features)

train_tabular_features = pd.DataFrame(train_features)
train_df = pd.concat([train_df, train_tabular_features], axis=1)
train_df = train_df.dropna()  # ìœ íš¨í•˜ì§€ ì•Šì€ SMILES ì œê±°

# Test ë°ì´í„° íŠ¹ì§• ê³„ì‚°
test_features = []
for smiles in test_df['SMILES']:
    features = calculate_molecular_features(smiles)
    test_features.append(features)

test_tabular_features = pd.DataFrame(test_features)
test_df = pd.concat([test_df, test_tabular_features], axis=1)
test_df = test_df.dropna()

print(f"\nìœ íš¨í•œ Train ë°ì´í„°: {train_df.shape}")
print(f"ìœ íš¨í•œ Test ë°ì´í„°: {test_df.shape}")

class CombinedBatchLoss(nn.Module):
    """
    Weighted MSEì™€ Correlation Lossë¥¼ ë°°ì¹˜ ë ˆë²¨ì—ì„œ ê²°í•©í•©ë‹ˆë‹¤.
    """
    def __init__(self, alpha=0.4, scaler=None, weight_configs={10: 5.0, 12: 20.0}):
        """
        alpha: WMSEì˜ ë¹„ì¤‘ (ëŒ€íšŒ ì§€í‘œì— ë§ì¶° 0.4 ê¶Œì¥)
        scaler: ë°ì´í„° ì •ê·œí™” ìŠ¤ì¼€ì¼ëŸ¬
        weight_configs: {threshold: weight} ê³ í™œì„± ë¶„ì ê°€ì¤‘ì¹˜ ì„¤ì •
        """
        super().__init__()
        self.alpha = alpha
        self.scaler = scaler
        self.weight_configs = weight_configs

    def forward(self, preds, targets):
        # --- 1. ê°€ì¤‘ì¹˜ ê³„ì‚° ---
        if self.scaler is not None:
            # ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ë³µì› (ê°€ì¤‘ì¹˜ ê³„ì‚°ì„ ìœ„í•¨). ë””ë°”ì´ìŠ¤ ì„¤ì • ì¤‘ìš”.
            mean = torch.tensor(self.scaler.mean_[0], device=preds.device)
            std = torch.tensor(self.scaler.scale_[0], device=preds.device)
            targets_orig = targets * std + mean
        else:
            targets_orig = targets

        weights = torch.ones_like(targets)
        # ê³ í™œì„± ë¶„ìì— ë†’ì€ ê°€ì¤‘ì¹˜ ì ìš© (10 ì´ìƒ 5ë°°, 12 ì´ìƒ 20ë°°)
        for threshold, weight in self.weight_configs.items():
            weights = torch.where(targets_orig > threshold, weight, weights)

        # --- 2. Weighted MSE (Batch Mean) ---
        mse = (preds - targets) ** 2
        weighted_mse = (mse * weights).mean() # ğŸ”¥ ë°°ì¹˜ í‰ê·  (ìŠ¤ì¹¼ë¼)

        # --- 3. Correlation Loss (Batch Level) ---
        eps = 1e-8
        preds_m = preds - preds.mean()
        targets_m = targets - targets.mean()
        
        # Pearson Correlation (R) ê³„ì‚°
        cov = (preds_m * targets_m).sum()
        preds_std = torch.sqrt((preds_m ** 2).sum() + eps)
        targets_std = torch.sqrt((targets_m ** 2).sum() + eps)
        
        corr = cov / (preds_std * targets_std + eps)
        # 1 - R ì‚¬ìš©
        corr_loss = 1.0 - torch.clamp(corr, -1.0, 1.0)

        # --- 4. ìµœì¢… ì†ì‹¤ ê²°í•© ---
        total_loss = self.alpha * weighted_mse + (1 - self.alpha) * corr_loss
        
        # ë¡œê¹…ì„ ìœ„í•´ ê°œë³„ ì†ì‹¤ë„ ë°˜í™˜ (í…ì„œ í˜•íƒœë¡œ ë°˜í™˜)
        return total_loss, weighted_mse, corr_loss

# Chemprop ë°ì´í„°í¬ì¸íŠ¸ ìƒì„±
def create_chemprop_datapoints(df, include_targets=True):
    """DataFrameì„ Chemprop MoleculeDatapoint ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    datapoints = []
    
    
    for idx, row in df.iterrows():
        smi = row['SMILES']
        
        if include_targets:
            pIC50_val = row['pIC50']
            
            datapoint = data.MoleculeDatapoint.from_smi(
                smi=smi,
                y=[pIC50_val]
            )
        else:
            datapoint = data.MoleculeDatapoint.from_smi(
                smi=smi
            )
        
        datapoints.append(datapoint)
    
    return datapoints

# ê¸°ì¡´ Tier ê´€ë ¨ ì½”ë“œë¥¼ ëª¨ë‘ ì‚­ì œí•˜ê³  ì•„ë˜ ì½”ë“œë¡œ ëŒ€ì²´

# === ğŸ”¥ ìƒˆë¡œìš´ ì½”ë“œ (ìˆ˜ë™ Binningìœ¼ë¡œ êµì²´) ===
def create_strat_key(df):
    # 1. pIC50 ê¸°ë°˜ ìˆ˜ë™ Binning ì •ì˜ (ê³ í™œì„± ì˜ì—­ ëª…í™•íˆ ë¶„ë¦¬)
    # Bins: <6, 6-8, 8-10, >10 (ê³ í™œì„±)
    bins = [0, 6, 8, 10, 14] 
    labels = [f'bin_{i}' for i in range(len(bins)-1)]
    pIC50_binned = pd.cut(df['pIC50'], bins=bins, labels=labels, include_lowest=True)
    
    # 2. ì¶œì²˜ì™€ Binì„ ê²°í•©
    return df['source'] + '_' + pIC50_binned.astype(str)

train_df['strat_key'] = create_strat_key(train_df)

# ì¶œì²˜ë³„ë¡œë§Œ ì¸µí™” ì¶”ì¶œ
train_indices = np.arange(len(train_df))
train_idx, val_idx = train_test_split(
    train_indices,
    test_size=0.2,
    stratify=train_df['strat_key'], # ìˆ˜ì •ëœ ì½”ë“œ
    random_state=SEED
)

train_split_df = train_df.iloc[train_idx].reset_index(drop=True)
val_split_df = train_df.iloc[val_idx].reset_index(drop=True)

val_split_df_temp = train_df.iloc[val_idx]
print(f"\n--- ë¶„í•  ê²€ì¦ ---")
print(f"Val Max pIC50: {val_split_df_temp['pIC50'].max():.2f}")
# ë¶„í•  ê²€ì¦
print(f"Train split: {len(train_split_df)}, Val split: {len(val_split_df)}")
print(f"\nTrain ì¶œì²˜ë³„ ë¹„ìœ¨:\n{train_split_df['source'].value_counts(normalize=True)}")
print(f"\nVal ì¶œì²˜ë³„ ë¹„ìœ¨:\n{val_split_df['source'].value_counts(normalize=True)}")

# Chemprop ëª¨ë¸ êµ¬ì¶•
# Message Passing ëª¨ë“ˆ (ë¶„ì êµ¬ì¡° ì¸ì½”ë”©)

# 1. CheMeleon Foundation Model ë¡œë“œ ë° featurizer ì •ì˜
if not os.path.exists("chemeleon_mp.pt"):
    urlretrieve(
        r"https://zenodo.org/records/15460715/files/chemeleon_mp.pt",
        "chemeleon_mp.pt",
    )

# Chemeleon ì˜ˆì œì— ë”°ë¼ featurizerì™€ mp ëª¨ë“ˆ ë¡œë“œ
featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()
chemeleon_mp_state = torch.load("chemeleon_mp.pt", weights_only=True)
mp = chemprop_nn.BondMessagePassing(**chemeleon_mp_state['hyper_parameters'])
mp.load_state_dict(chemeleon_mp_state['state_dict'])
print("Chemeleon Foundation Model ë¡œë“œ ì™„ë£Œ.")

# ğŸ”¥ ìˆ˜ì •: CheMeleonì€ freeze ì˜µì…˜ ì¶”ê°€
for param in mp.parameters():
    param.requires_grad = True # CheMeleon ê°€ì¤‘ì¹˜ ê³ ì •
mp.eval()  # Dropout ë“± ë¹„í™œì„±í™”

# 2. Chemprop ë°ì´í„°ì…‹ ìƒì„± (Featurizer ëª…ì‹œì  ì „ë‹¬)
train_datapoints = create_chemprop_datapoints(train_split_df)
val_datapoints = create_chemprop_datapoints(val_split_df)

train_dset = data.MoleculeDataset(train_datapoints, featurizer=featurizer)
scaler = train_dset.normalize_targets()
val_dset = data.MoleculeDataset(val_datapoints, featurizer=featurizer)
val_dset.normalize_targets(scaler)

# 3. ë°ì´í„°ë¡œë” ìƒì„±
train_loader = data.build_dataloader(train_dset, batch_size=32, num_workers=0)
val_loader = data.build_dataloader(val_dset, batch_size=32, shuffle=False, num_workers=0)


# ğŸ”¥ ìˆ˜ì •ëœ ImprovedChempropModule
class FineTunedChempropModule(pl.LightningModule):
    def __init__(self, model, loss_function, warmup_epochs=5, base_lr=5e-4, 
                 chemeleon_lr_factor=0.1, scaler=None):
        super().__init__()
        self.model = model
        self.loss_function = loss_function # ğŸ”¥
        self.warmup_epochs = warmup_epochs
        self.base_lr = base_lr
        self.chemeleon_lr_factor = chemeleon_lr_factor  # CheMeleonì€ ë” ë‚®ì€ lr
        self.current_lr = base_lr
        self.scaler = scaler

        self.validation_step_outputs = []
        
        
    def forward(self, bmg, V_d=None, X_d=None):
        return self.model(bmg, V_d, X_d)
    

    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        """
        trainer.predict() í˜¸ì¶œ ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤.
        batch ê°ì²´ë¥¼ ì˜¬ë°”ë¥´ê²Œ í•´ì²´í•˜ì—¬ forwardë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        """
        # TrainingBatch ê°ì²´ì—ì„œ í•„ìš”í•œ êµ¬ì„± ìš”ì†Œ ì¶”ì¶œ
        bmg = batch.bmg
        
        # ì¶”ê°€ íŠ¹ì§•(V_d, X_d)ì´ ìˆë‹¤ë©´ ì²˜ë¦¬ (ì•ˆì „í•œ ì ‘ê·¼ì„ ìœ„í•´ hasattr ì‚¬ìš©)
        V_d = batch.V_d if hasattr(batch, 'V_d') else None
        X_d = batch.X_d if hasattr(batch, 'X_d') else None

        # í•´ì²´ëœ êµ¬ì„± ìš”ì†Œë¡œ forward ë©”ì„œë“œ í˜¸ì¶œ
        return self.forward(bmg, V_d, X_d)

    
    def training_step(self, batch, batch_idx):
        # Forward pass
        preds = self.forward(batch.bmg, batch.V_d, batch.X_d)
        # mask = torch.ones(preds.shape[0], dtype=torch.bool, device=preds.device)

        # ğŸ”¥ ë°°ì¹˜ ë ˆë²¨ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©
        loss, wmse_part, corr_part = self.loss_function(preds, batch.Y)

        self.log('train_loss', loss, on_epoch=True, prog_bar=True)
        self.log('train/wmse_part', wmse_part, on_epoch=True)
        self.log('train/corr_part', corr_part, on_epoch=True)
        self.log('lr', self.current_lr, on_step=False, on_epoch=True, prog_bar=True)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        # ê¸°ì¡´ê³¼ ë™ì¼
        preds = self.forward(batch.bmg, batch.V_d, batch.X_d)
        loss, _, _ = self.loss_function(preds, batch.Y)
        
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)

        # ğŸ”¥ ê²€ì¦ í†µê³„ í™•ì¸ì„ ìœ„í•´ ê²°ê³¼ ì €ì¥ (ì •ê·œí™”ëœ ìƒíƒœ)
        self.validation_step_outputs.append({'preds': preds.detach(), 'targets': batch.Y.detach()})

        return loss
    
        # ğŸ”¥ on_validation_epoch_end ìˆ˜ì • (ì „ì²´ í†µê³„ ê³„ì‚° ë° ì¶œë ¥)
    def on_validation_epoch_end(self):
        if not self.validation_step_outputs:
            return
        
        # 1. ì „ì²´ ê²€ì¦ ë°°ì¹˜ ê²°ê³¼ ì§‘ê³„
        all_preds_norm = torch.cat([x['preds'] for x in self.validation_step_outputs]).squeeze()
        all_targets_norm = torch.cat([x['targets'] for x in self.validation_step_outputs]).squeeze()
        
        # 2. ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ì—­ë³€í™˜ (í†µê³„ í™•ì¸ìš©)
        # ìŠ¤ì¼€ì¼ëŸ¬ ê°’ì„ í…ì„œë¡œ ë³€í™˜í•˜ê³  ë””ë°”ì´ìŠ¤ ë§ì¶”ê¸°
        mean = torch.tensor(self.scaler.mean_[0], device=self.device)
        std = torch.tensor(self.scaler.scale_[0], device=self.device)
        
        preds_orig = all_preds_norm * std + mean
        targets_orig = all_targets_norm * std + mean
        
        # 3. ì „ì²´ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ í†µê³„ ì¶œë ¥
        print(f"\n=== Epoch {self.current_epoch} Validation Summary (ì „ì²´ ë°°ì¹˜) ===")
        print(f"Preds ë²”ìœ„:   [{preds_orig.min():.2f}, {preds_orig.max():.2f}]")
        # ì´ì œ Targets ìµœëŒ€ê°’ì´ 13.0 ê·¼ì²˜ë¡œ ë³´ì—¬ì•¼ í•©ë‹ˆë‹¤.
        print(f"Targets ë²”ìœ„: [{targets_orig.min():.2f}, {targets_orig.max():.2f}]") 
        print(f"ì˜ˆì¸¡ ë‹¤ì–‘ì„± (std): {preds_orig.std():.3f}")
        
        # 4. ì €ì¥ì†Œ ì´ˆê¸°í™”
        self.validation_step_outputs.clear()
    
    
    def on_train_epoch_start(self):
        # Warmup logic
        if self.current_epoch < self.warmup_epochs:
            # Linear warmup
            lr = self.base_lr * 0.5 * (self.current_epoch + 1) / self.warmup_epochs
            self.current_lr = lr
            
            # ëª¨ë“  param groupì— ì ìš©
            for pg in self.optimizers().param_groups:
                if 'initial_lr' not in pg:
                    pg['initial_lr'] = pg['lr']
                pg['lr'] = pg['initial_lr'] * (self.current_epoch + 1) / self.warmup_epochs
            
            print(f"Warmup Epoch {self.current_epoch}: Base LR = {lr:.6f}")


    def on_train_epoch_end(self):
        # Update current_lr for logging
        self.current_lr = self.optimizers().param_groups[0]['lr']
        
        # ğŸ”¥ ì¶”ê°€: í•™ìŠµë¥  ë³€í™” ëª¨ë‹ˆí„°ë§
        if self.current_epoch % 10 == 0:
            print(f"\n=== í•™ìŠµë¥  í˜„í™© (Epoch {self.current_epoch}) ===")
            for i, pg in enumerate(self.optimizers().param_groups):
                print(f"Param Group {i}: lr = {pg['lr']:.2e}")

    def configure_optimizers(self):
        # ğŸ”¥ ì°¨ë“± í•™ìŠµë¥  ì„¤ì •
        param_groups = []
        
        # CheMeleon íŒŒë¼ë¯¸í„° (ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ )
        chemeleon_params = []
        ffn_params = []
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                if 'message_passing' in name:  # CheMeleon ë¶€ë¶„
                    chemeleon_params.append(param)
                else:  # FFN ë¶€ë¶„
                    ffn_params.append(param)
        
        if chemeleon_params:
            param_groups.append({
                'params': chemeleon_params,
                'lr': self.base_lr * self.chemeleon_lr_factor,  # 0.01x
                'weight_decay': 1e-5
            })
        
        param_groups.append({
            'params': ffn_params,
            'lr': self.base_lr,
            'weight_decay': 5e-4
        })
        print(f"FFN params: {len(ffn_params)} tensors")

        
        # Optimizer
        optimizer = torch.optim.AdamW(
            param_groups,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        # Scheduler - Cosine Annealing (ë¯¸ì„¸ì¡°ì •ì— ì í•©)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=50,  # 50 epochs ì£¼ê¸°
            eta_min=1e-6
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "interval": "epoch",
                "frequency": 1
            }
        }

# ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: Lightning Module Wrapper
class ImprovedChempropModule(pl.LightningModule):
    def __init__(self, model, criterion, warmup_epochs=3, initial_lr=1e-4, scaler=None):
        super().__init__()
        self.model = model
        self.criterion = criterion  # ì†ì‹¤ í•¨ìˆ˜ ë³„ë„ ì „ë‹¬
        self.warmup_epochs = warmup_epochs
        self.initial_lr = initial_lr
        self.current_lr = initial_lr
        self.scaler = scaler  # ğŸ”¥ scaler ì¶”ê°€

        # ğŸ”¥ ì¶”ê°€: ì†ì‹¤ ì¶”ì 
        self.val_losses_per_sample = []
        
    def forward(self, bmg, V_d=None, X_d=None):
        # ëª¨ë¸ì€ ì˜ˆì¸¡ê°’ë§Œ ë°˜í™˜
        return self.model(bmg, V_d, X_d)
    
    def training_step(self, batch, batch_idx):
        # Forward pass - ì˜ˆì¸¡ê°’ë§Œ ë°›ìŒ
        preds = self.forward(
            batch.bmg,
            batch.V_d,
            batch.X_d
        )

        mask = torch.ones(preds.shape[0], dtype=torch.bool, device=preds.device)

        # ğŸ”¥ ì—¬ê¸°ì— ì¶”ê°€! (ì²« ë²ˆì§¸ ì—í¬í¬, ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œë§Œ)
        if self.current_epoch == 0 and batch_idx == 0:
            # output transformì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
            print(f"\n=== Output Transform í™•ì¸ ===")
            print(f"Scaler mean: {self.scaler.mean_[0]:.2f}, std: {self.scaler.scale_[0]:.2f}")
            print(f"ì •ê·œí™”ëœ ì˜ˆì¸¡: {preds[:5].squeeze().tolist()}")
            
            # ìˆ˜ë™ìœ¼ë¡œ ì—­ë³€í™˜
            manual_denorm = preds * self.scaler.scale_[0] + self.scaler.mean_[0]
            print(f"ìˆ˜ë™ ì—­ë³€í™˜: {manual_denorm[:5].squeeze().tolist()}")
            
            # íƒ€ê²Ÿë„ í™•ì¸
            print(f"ì •ê·œí™”ëœ íƒ€ê²Ÿ: {batch.Y[:5].squeeze().tolist()}")
            targets_denorm = batch.Y * self.scaler.scale_[0] + self.scaler.mean_[0]
            print(f"ì›ë³¸ íƒ€ê²Ÿ: {targets_denorm[:5].squeeze().tolist()}")

        
        # Loss ê³„ì‚° - criterion ì‚¬ìš©
        loss = self.criterion._calc_unreduced_loss(
            preds=preds,
            targets=batch.Y,
            mask=mask,  # mask ìƒì„±
            weights=batch.w if hasattr(batch, 'w') and batch.w is not None else torch.ones_like(batch.Y),
            lt_mask=batch.lt_mask if hasattr(batch, 'lt_mask') else torch.zeros_like(mask),
            gt_mask=batch.gt_mask if hasattr(batch, 'gt_mask') else torch.zeros_like(mask)
        )

        # í‰ê·  ê³„ì‚°
        loss = loss.mean()
        
        # Logging
        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log('lr', self.current_lr, on_step=False, on_epoch=True, prog_bar=True)
        
        return loss
    
    def validation_step(self, batch, batch_idx):
        # Forward pass - ì˜ˆì¸¡ê°’ë§Œ ë°›ìŒ
        preds = self.forward(
            batch.bmg,
            batch.V_d,
            batch.X_d
        )

        # mask ìƒì„±
        mask = torch.ones(preds.shape[0], dtype=torch.bool, device=preds.device)
        
        # ğŸ”¥ ê°œë³„ ìƒ˜í”Œë³„ ì†ì‹¤ ê³„ì‚°
        individual_losses = self.criterion._calc_unreduced_loss(
            preds=preds,
            targets=batch.Y,
            mask=mask,
            weights=batch.w if hasattr(batch, 'w') and batch.w is not None else torch.ones_like(batch.Y),
            lt_mask=batch.lt_mask if hasattr(batch, 'lt_mask') else torch.zeros_like(mask),
            gt_mask=batch.gt_mask if hasattr(batch, 'gt_mask') else torch.zeros_like(mask)
        )
        
        # ğŸ”¥ ê°œë³„ ì†ì‹¤ ì €ì¥
        self.val_losses_per_sample.extend(individual_losses.detach().cpu().numpy())
        
        # í‰ê·  ê³„ì‚°
        loss = individual_losses.mean()
        
        # Logging
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)

        
        # validation_stepì— ì¶”ê°€
        if batch_idx == 0:
            # ì›ë³¸ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            preds_orig = preds
            targets_orig = batch.Y * self.scaler.scale_[0] + self.scaler.mean_[0]
            
            print(f"\nì›ë³¸ ìŠ¤ì¼€ì¼:")
            print(f"Preds ë²”ìœ„: [{preds_orig.min():.1f}, {preds_orig.max():.1f}]")
            print(f"Targets ë²”ìœ„: [{targets_orig.min():.1f}, {targets_orig.max():.1f}]")
            print(f"ì˜ˆì¸¡ ë‹¤ì–‘ì„±: {preds.std():.3f}")
        
        return loss
    
    def on_validation_epoch_end(self):
        # ğŸ”¥ ì†ì‹¤ ë¶„í¬ ë¶„ì„
        if self.val_losses_per_sample:
            losses = np.array(self.val_losses_per_sample)
            print(f"\n=== Val Loss ë¶„ì„ ===")
            print(f"í‰ê· : {losses.mean():.2f}")
            print(f"ì¤‘ì•™ê°’: {np.median(losses):.2f}")
            print(f"ìµœëŒ€ê°’: {losses.max():.2f}")
            print(f"ì†ì‹¤ > 10ì¸ ìƒ˜í”Œ ìˆ˜: {(losses > 10).sum()}")
            print(f"ì†ì‹¤ > 50ì¸ ìƒ˜í”Œ ìˆ˜: {(losses > 50).sum()}")
            
            # ì´ˆê¸°í™”
            self.val_losses_per_sample = []
    
    def on_train_epoch_start(self):
        # Warmup logic
        if self.current_epoch < self.warmup_epochs:
            # Linear warmup
            lr = self.initial_lr * 0.5 * (self.current_epoch + 1) / self.warmup_epochs
            self.current_lr = lr
            for pg in self.optimizers().param_groups:
                pg['lr'] = lr
            print(f"Warmup Epoch {self.current_epoch}: LR = {lr:.6f}")
    
    def configure_optimizers(self):
        # Optimizer
        optimizer = torch.optim.Adam(
            self.parameters(),
            lr=self.initial_lr,
            weight_decay=5e-4,
            betas=(0.9, 0.999),
            eps=1e-8  # ğŸ”¥ ì•ˆì •ì„± ì¶”ê°€
        )
        
        # Scheduler - ReduceLROnPlateau
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='min',
            factor=0.3,      # 0.5 â†’ 0.3 (ë” ê¸‰ê²©í•œ ê°ì†Œ)
            patience=3,      # ê·¸ëŒ€ë¡œ ìœ ì§€
            threshold=0.01,  # ê·¸ëŒ€ë¡œ ìœ ì§€
            min_lr=1e-6,     # ì¶”ê°€: ìµœì†Œ LR ì„¤ì •
            cooldown=2,      # ì¶”ê°€: LR ë³€ê²½ í›„ ëŒ€ê¸°
        )
        
        return {
            "optimizer": optimizer,
            "lr_scheduler": {
                "scheduler": scheduler,
                "monitor": "val_loss",  # ì¶”ê°€: ëª…ì‹œì  ì§€ì •
                "interval": "epoch",
                "frequency": 1          # ì¶”ê°€: ë§¤ epochë§ˆë‹¤
            }
        }
    
    def on_train_epoch_end(self):
        # Update current_lr for logging
        self.current_lr = self.optimizers().param_groups[0]['lr']

# 4. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” ê°œì„ 
def init_model_weights(module):
    if isinstance(module, nn.Linear):
        if module.out_features == 1:  # ìµœì¢… ì¶œë ¥ì¸µ
            # í‰ê·  0 ê·¼ì²˜ë¡œ ì´ˆê¸°í™” (ì •ê·œí™”ëœ ê³µê°„ì—ì„œ)
            nn.init.normal_(module.weight, mean=0.0, std=0.01)
            if module.bias is not None:
                # íƒ€ê²Ÿ í‰ê· ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
                nn.init.constant_(module.bias, 0.0)  # ì •ê·œí™”ëœ í‰ê· ì€ 0
        else:
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)

# 4. Chemprop ëª¨ë¸ êµ¬ì¶•
# Aggregation
agg = chemprop_nn.MeanAggregation()

# ğŸ”¥ ì†ì‹¤ í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤í™” (WMSE 40%, CorrLoss 60%)
combined_loss_fn = CombinedBatchLoss(
    alpha=0.4, 
    scaler=scaler, 
    weight_configs={10: 5, 12: 10}
)


ffn = chemprop_nn.RegressionFFN(
    input_dim=mp.output_dim,  # CheMeleonì˜ ì¶œë ¥ ì°¨ì› ì‚¬ìš© (2048)
    hidden_dim=1024,  # 300 â†’ 1024
    n_layers=3,       # 2 â†’ 3
    dropout=0.3,      # 0.0 â†’ 0.1
)

# ìƒˆë¡œ ì¶”ê°€: FFNë§Œ ì´ˆê¸°í™” (CheMeleonì€ ì œì™¸)
ffn.apply(init_model_weights)

# ìµœì¢… MPNN ëª¨ë¸
model = models.MPNN(mp, agg, ffn, batch_norm=False)  # batch_norm=False ì¶”ê°€


# ğŸ”¥ ìˆ˜ì •: ë¯¸ì„¸ì¡°ì •ìš© Lightning Module ì‚¬ìš©
lightning_model = FineTunedChempropModule(
    model=model,
    loss_function=combined_loss_fn, # ğŸ”¥ ì†ì‹¤ í•¨ìˆ˜ ì „ë‹¬
    warmup_epochs=2,  # warmup ì—°ì¥
    base_lr=3e-4,  # ê¸°ë³¸ í•™ìŠµë¥  ë‚®ì¶¤
    chemeleon_lr_factor=0.01,  # CheMeleonì€ 1/100 í•™ìŠµë¥ 
    scaler=scaler
)

# %%
# ëª¨ë¸ í•™ìŠµ
print("Chemprop ëª¨ë¸ í•™ìŠµ ì¤‘...")

# Logger ì„¤ì •
logger = TensorBoardLogger(
    save_dir='./logs',
    name='chemprop_training',
    version='v1'
)

# ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: Callbacks
callbacks = [
    # Early Stopping
    EarlyStopping(
        monitor='val_loss',
        min_delta=0.0005,
        patience=15,
        verbose=True,
        mode='min',
        strict=True,       # ì¶”ê°€: ì—„ê²©í•œ ëª¨ë“œ
        check_finite=True  # ì¶”ê°€: NaN/Inf ì²´í¬
    ),
    # Learning Rate Monitor
    LearningRateMonitor(logging_interval='epoch'),
    # Model Checkpoint (ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥)
    ModelCheckpoint(
        monitor='val_loss',
        dirpath='./chemprop_finetuned_checkpoints',
        filename='chemprop-{epoch:02d}-{val_loss:.4f}',
        save_top_k=1,
        mode='min',
        save_weights_only=True
    )
]


trainer = pl.Trainer(
    max_epochs=150,  # ë” ë§ì€ epoch (early stoppingì´ ìˆìœ¼ë¯€ë¡œ)
    accelerator='auto',
    devices=1,
    enable_progress_bar=True,
    logger=logger,  # ë˜ëŠ” TensorBoardLogger ì‚¬ìš© ê°€ëŠ¥
    enable_checkpointing=True,
    callbacks=callbacks,
    gradient_clip_val=0.5,  # Gradient clipping
    gradient_clip_algorithm='norm',
    deterministic=True,  # ì¬í˜„ì„±
    precision=32  # Mixed precision training (ì„ íƒì‚¬í•­)
)

# í•™ìŠµ ì‹¤í–‰
trainer.fit(lightning_model, train_loader, val_loader)


# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ
best_model_path = trainer.checkpoint_callback.best_model_path
if best_model_path:
    print(f"ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ: {best_model_path}")
    checkpoint = torch.load(best_model_path)
    lightning_model.load_state_dict(checkpoint['state_dict'])
    model = lightning_model.model  # ì›ë˜ ëª¨ë¸ ì¶”ì¶œ

    # ğŸ”¥ ì¶”ê°€: ìµœê³  ëª¨ë¸ì„ ë³„ë„ë¡œ ì €ì¥
    torch.save({
        'model_state_dict': model.state_dict(),
        'scaler': scaler,
        'config': {
            'hidden_dim': 1024,
            'n_layers': 3,
            'dropout': 0.3
        }
    }, './best_chemprop_model.pt')

print("í•™ìŠµ ì™„ë£Œ!")

# ## ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ë° ì„ë² ë”© ì¶”ì¶œ

# ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
checkpoint_path = './best_chemprop_model.pt'

if os.path.exists(checkpoint_path):
    print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: {checkpoint_path}")
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu', weights_only=False)
    
    # ì„¤ì • ì •ë³´ ì¶”ì¶œ
    config = checkpoint.get('config', {
        'hidden_dim': 1024,
        'n_layers': 3,
        'dropout': 0.3
    })
    scaler = checkpoint['scaler']
    
    print(f"ëª¨ë¸ ì„¤ì •: {config}")
    print(f"Scaler - mean: {scaler.mean_[0]:.3f}, std: {scaler.scale_[0]:.3f}")
    
    # ëª¨ë¸ ì¬êµ¬ì„±
    # 1. Featurizer
    featurizer = featurizers.SimpleMoleculeMolGraphFeaturizer()
    
    # 2. CheMeleon MP ë¡œë“œ
    if os.path.exists("chemeleon_mp.pt"):
        chemeleon_mp_state = torch.load("chemeleon_mp.pt", weights_only=True)
        mp = chemprop_nn.BondMessagePassing(**chemeleon_mp_state['hyper_parameters'])
        mp.load_state_dict(chemeleon_mp_state['state_dict'])
        
        # Freeze
        for param in mp.parameters():
            param.requires_grad = True
        mp.eval()
    else:
        raise FileNotFoundError("CheMeleon ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: chemeleon_mp.pt")
    
    # 3. Aggregation
    agg = chemprop_nn.MeanAggregation()

    # ğŸ”¥ ì†ì‹¤ í•¨ìˆ˜ ì¸ìŠ¤í„´ìŠ¤í™” (WMSE 40%, CorrLoss 60%)
    combined_loss_fn = CombinedBatchLoss(
        alpha=0.4, 
        scaler=scaler, 
        weight_configs={10: 5, 12: 10}
    )
    
    # 6. FFN
    ffn = chemprop_nn.RegressionFFN(
        input_dim=mp.output_dim,
        hidden_dim=config['hidden_dim'],
        n_layers=config['n_layers'],
        dropout=config['dropout'],
    )
    
    # 7. ì „ì²´ ëª¨ë¸ êµ¬ì„±
    model = models.MPNN(mp, agg, ffn, batch_norm=False)
    
    # 8. ì²´í¬í¬ì¸íŠ¸ì—ì„œ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # Device ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    
    print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    
else:
    raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")

# ì„ë² ë”© ì¶”ì¶œ í•¨ìˆ˜
def extract_embeddings(model, datapoints, featurizer_instance):
    """í•™ìŠµëœ ëª¨ë¸ì—ì„œ ë¶„ì ì„ë² ë”© ì¶”ì¶œ"""
    embedding_dset = data.MoleculeDataset(datapoints, featurizer=featurizer_instance)
    loader = data.build_dataloader(
        embedding_dset,  # featurizer ì „ë‹¬
        batch_size=64, 
        shuffle=False, 
        num_workers=0
    )
    
    device = next(model.parameters()).device
    model.eval()

    # ì¶”ê°€: CheMeleon MPë„ í‰ê°€ ëª¨ë“œ í™•ì¸
    if hasattr(model, 'message_passing'):
        model.message_passing.eval()

    embeddings = []
    
    with torch.no_grad():
        for batch in loader:
            batch.bmg.to(device)
            if batch.V_d is not None:
                batch.V_d = batch.V_d.to(device)

            # FFNì˜ ë§ˆì§€ë§‰ ì€ë‹‰ì¸µ ì¶œë ¥ì„ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©
            encoding = model.encoding(batch.bmg, batch.V_d, X_d=None, i=-1)
            embeddings.append(encoding.cpu().numpy())
    
    model.train()
    
    return np.concatenate(embeddings, axis=0)

# ì „ì²´ train ë°ì´í„°ì˜ ì„ë² ë”© ì¶”ì¶œ
all_train_datapoints = create_chemprop_datapoints(train_df)
train_embeddings = extract_embeddings(model, all_train_datapoints, featurizer)

# Test ë°ì´í„°ì˜ ì„ë² ë”© ì¶”ì¶œ
test_datapoints = create_chemprop_datapoints(test_df, include_targets=False)
test_embeddings = extract_embeddings(model, test_datapoints, featurizer)

print(f"Train embeddings shape: {train_embeddings.shape}")
print(f"Test embeddings shape: {test_embeddings.shape}")

# ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: ë©”ëª¨ë¦¬ ì •ë¦¬
import gc
torch.cuda.empty_cache()
gc.collect()

# ğŸ”¥ ìƒˆë¡œ ì¶”ê°€: ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ì •ë¦¬ (ì„ íƒì‚¬í•­)
import shutil

if os.path.exists('./chemprop_checkpoints'):
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ
    print("ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì¤‘...")
    shutil.rmtree('./chemprop_checkpoints')  # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ

# ## 3. Phase 3: ìµœì¢… LightGBM ëª¨ë¸ í•™ìŠµ

# %%
# ìµœì¢… íŠ¹ì§•ì…‹ êµ¬ì¶• (ì„ë² ë”© + í…Œì´ë¸” íŠ¹ì§•)
feature_cols = ['MolWt', 'LogP', 'NumHDonors', 'NumHAcceptors', 
               'NumRotatableBonds', 'TPSA', 'QED', 'NumHeteroatoms',
               'RingCount', 'FractionCsp3']

# íŠ¹ì§• ì •ê·œí™”
from sklearn.preprocessing import StandardScaler as TabularScaler
feature_scaler = TabularScaler()
train_tabular_scaled = feature_scaler.fit_transform(train_df[feature_cols])
test_tabular_scaled = feature_scaler.transform(test_df[feature_cols])

# ì„ë² ë”©ê³¼ í…Œì´ë¸” íŠ¹ì§• ê²°í•©
X_train = np.hstack([train_embeddings, train_tabular_scaled])
y_train = train_df['pIC50'].values


X_test = np.hstack([test_embeddings, test_tabular_scaled])

print(f"ìµœì¢… Train íŠ¹ì§• shape: {X_train.shape}")
print(f"ìµœì¢… Test íŠ¹ì§• shape: {X_test.shape}")

# Phase 3 ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€
def high_activity_mixup(X, y, high_activity_threshold=10, alpha=0.2, n_mixup_pairs=50):
    """
    ê³ í™œì„± ë¶„ìë¼ë¦¬ë§Œ Mixup ìˆ˜í–‰
    
    Parameters:
    - X: íŠ¹ì§• í–‰ë ¬
    - y: íƒ€ê²Ÿ ê°’
    - high_activity_threshold: ê³ í™œì„± ê¸°ì¤€ (ê¸°ë³¸ 10)
    - alpha: Beta ë¶„í¬ íŒŒë¼ë¯¸í„° (ì‘ì„ìˆ˜ë¡ ì›ë³¸ì— ê°€ê¹Œì›€)
    - n_mixup_pairs: ìƒì„±í•  mixup ìŒì˜ ìˆ˜
    """
    high_mask = y > high_activity_threshold
    high_indices = np.where(high_mask)[0]
    
    print(f"  ê³ í™œì„± ìƒ˜í”Œ ìˆ˜: {len(high_indices)}")
    
    if len(high_indices) < 2:
        print("  ê³ í™œì„± ìƒ˜í”Œì´ 2ê°œ ë¯¸ë§Œì´ë¼ Mixup ë¶ˆê°€")
        return X, y
    
    # ì‹¤ì œ ìƒì„±í•  mixup ìˆ˜ ê²°ì •
    n_pairs = min(n_mixup_pairs, len(high_indices) * 2)  # ë„ˆë¬´ ë§ì´ ë§Œë“¤ì§€ ì•Šë„ë¡
    
    X_mixed = []
    y_mixed = []
    
    for _ in range(n_pairs):
        # ê³ í™œì„± ìƒ˜í”Œ ì¤‘ì—ì„œ 2ê°œ ì„ íƒ
        idx1, idx2 = np.random.choice(high_indices, 2, replace=True)  # replace=Trueë¡œ ë” ë‹¤ì–‘í•œ ì¡°í•©
        
        # Beta ë¶„í¬ì—ì„œ lambda ìƒ˜í”Œë§
        lam = np.random.beta(alpha, alpha)
        
        # ìƒˆë¡œìš´ ìƒ˜í”Œ ìƒì„±
        x_new = lam * X[idx1] + (1 - lam) * X[idx2]
        y_new = lam * y[idx1] + (1 - lam) * y[idx2]
        
        X_mixed.append(x_new)
        y_mixed.append(y_new)
    
    # ì›ë³¸ ë°ì´í„°ì™€ ê²°í•©
    X_augmented = np.vstack([X, np.array(X_mixed)])
    y_augmented = np.hstack([y, np.array(y_mixed)])
    
    print(f"  Mixupìœ¼ë¡œ {n_pairs}ê°œ ìƒ˜í”Œ ì¶”ê°€")
    
    return X_augmented, y_augmented

# CatBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°
cat_params = {
    'iterations': 500,
    'learning_rate': 0.05,
    'depth': 6,
    'l2_leaf_reg': 3,
    'random_state': SEED,
    'verbose': False,
    'early_stopping_rounds': 50,
    'task_type': 'GPU',  # GPU ì‚¬ìš©ì‹œ 'GPU'ë¡œ ë³€ê²½
    'loss_function': 'RMSE',
    'eval_metric': 'RMSE',
}

# ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ ì •ì˜
def calculate_sample_weights(y, high_weight=5.0, very_high_weight=15.0):
    weights = np.ones_like(y)
    weights[y > 10] = high_weight
    weights[y > 12] = very_high_weight
    return weights


# %%
from sklearn.model_selection import GroupKFold

# 5-Fold CVë¡œ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
gkf = GroupKFold(n_splits=5) 
groups = train_df['GroupID'].values # ğŸ”¥ Phase 1ì—ì„œ ìƒì„±í•œ ê·¸ë£¹ ì •ë³´ ì‚¬ìš©
cv_scores = []
cv_r2_scores = []
oof_predictions = np.zeros(len(X_train))

for fold, (train_idx, val_idx) in enumerate(gkf.split(X_train, y_train, groups=groups)): # ë³€ê²½
    print(f"\nFold {fold + 1}/5")
    
    X_tr, X_val = X_train[train_idx], X_train[val_idx]
    y_tr, y_val = y_train[train_idx], y_train[val_idx]
    
    # ê³ í™œì„± ìƒ˜í”Œë§Œ Mixup ì ìš©
    print(f"  Mixup ì „: {len(X_tr)} ìƒ˜í”Œ")
    
    # ê³ í™œì„± ìƒ˜í”Œë¼ë¦¬ë§Œ Mixup
    X_tr, y_tr = high_activity_mixup(
        X_tr, y_tr, 
        high_activity_threshold=10,  # pIC50 > 10
        alpha=0.2,  # 0.2 ì •ë„ê°€ ì ë‹¹ (ë„ˆë¬´ ì‘ìœ¼ë©´ ì›ë³¸ê³¼ ë„ˆë¬´ ë¹„ìŠ·)
        n_mixup_pairs=30  # fold ë³„ë¡œ 30ê°œ ì •ë„ ìƒì„±
    )
    
    print(f"  Mixup í›„: {len(X_tr)} ìƒ˜í”Œ")
    
    # CatBoost Pool ìƒì„±
    train_weights = calculate_sample_weights(y_tr)
    train_pool = Pool(X_tr, y_tr, weight=train_weights) # ğŸ”¥ weight ì ìš© í™•ì¸
    val_pool = Pool(X_val, y_val)
    
    # ëª¨ë¸ í•™ìŠµ
    model = CatBoostRegressor(**cat_params)
    model.fit(
        train_pool,
        eval_set=val_pool,
        verbose=100,
        plot=False
    )
    
    # ì˜ˆì¸¡
    y_pred = model.predict(X_val)
    oof_predictions[val_idx] = y_pred
    
    # í‰ê°€
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    r2 = r2_score(y_val, y_pred)
    
    cv_scores.append(rmse)
    cv_r2_scores.append(r2)
    
    print(f"  RMSE: {rmse:.4f}, R2: {r2:.4f}")

print(f"\ní‰ê·  CV RMSE: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
print(f"í‰ê·  CV R2: {np.mean(cv_r2_scores):.4f} Â± {np.std(cv_r2_scores):.4f}")

# ëŒ€íšŒ í‰ê°€ ì§€í‘œ ì •í™•íˆ êµ¬í˜„
def competition_score(y_true, y_pred):
    """ëŒ€íšŒ í‰ê°€ ì§€í‘œ ê³„ì‚°"""
    # A: Normalized RMSE
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    y_range = y_true.max() - y_true.min()
    normalized_rmse = rmse / y_range if y_range > 0 else rmse
    A = 1 - min(normalized_rmse, 1)
    
    # B: RÂ² on pIC50 scale
    B = r2_score(y_true, y_pred)
    
    # Final score
    score = 0.4 * A + 0.6 * B
    
    return score, A, B

# ëŒ€íšŒ í‰ê°€ ì§€í‘œë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
comp_score, A, B = competition_score(y_train, oof_predictions)
print(f"Competition Score: {comp_score:.4f} (A: {A:.4f}, B: {B:.4f})")

# ## 4. Phase 4: ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡

# ì „ì²´ ë°ì´í„°ë¡œ ìµœì¢… ëª¨ë¸ í•™ìŠµ
print("\nìµœì¢… CatBoost ëª¨ë¸ í•™ìŠµ ì¤‘...")

# ì „ì²´ ë°ì´í„°ì— ê³ í™œì„± Mixupë§Œ ì ìš©
X_train_final = X_train.copy()
y_train_final = y_train.copy()

# ê³ í™œì„± ìƒ˜í”Œë¼ë¦¬ë§Œ Mixup
X_train_final, y_train_final = high_activity_mixup(
    X_train_final, y_train_final,
    high_activity_threshold=10,
    alpha=0.2,
    n_mixup_pairs=200  # ì „ì²´ ë°ì´í„°ë‹ˆê¹Œ ë” ë§ì´ ìƒì„±
)

print(f"ìµœì¢… í•™ìŠµ ë°ì´í„°: {len(X_train_final)} ìƒ˜í”Œ (ì›ë³¸: {len(X_train)})")

# ğŸ”¥ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì—ë„ ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ì ìš©
final_weights = calculate_sample_weights(y_train_final)
train_pool_full = Pool(X_train_final, y_train_final, weight=final_weights) # ğŸ”¥ weight ì ìš© í™•ì¸

# ìµœì¢… ëª¨ë¸ í•™ìŠµ
final_model = CatBoostRegressor(
    iterations=500,  # ì „ì²´ ë°ì´í„°ì´ë¯€ë¡œ ë” ë§ì€ iteration
    learning_rate=0.03,  # ì•½ê°„ ë‚®ì¶°ì„œ ì„¸ë°€í•˜ê²Œ í•™ìŠµ
    depth=6,
    l2_leaf_reg=3,
    random_state=SEED,
    verbose=100,
    task_type='GPU'
)

final_model.fit(
    train_pool_full,
    verbose=100,
    plot=False
)

# Test ë°ì´í„° ì˜ˆì¸¡
test_pIC50_pred = final_model.predict(X_test)

# IC50(nM) ë³€í™˜
test_IC50_nM = 10 ** (9 - test_pIC50_pred)

# %%
# ì œì¶œ íŒŒì¼ ìƒì„±
submission_df = pd.DataFrame({
    'ID': test_df['ID'],
    'ASK1_IC50_nM': test_IC50_nM
})

# TEST_015, TEST_016, TEST_017 í™•ì¸ (í•µì‹¬ ë¶„ì)
key_molecules = ['TEST_015', 'TEST_016', 'TEST_017']
key_predictions = submission_df[submission_df['ID'].isin(key_molecules)]
print("\ní•µì‹¬ ë¶„ì ì˜ˆì¸¡ê°’:")
print(key_predictions)
print(f"\ní•µì‹¬ ë¶„ì í‰ê·  pIC50: {np.mean(9 - np.log10(key_predictions['ASK1_IC50_nM'])):.2f}")

# ì „ì²´ ì˜ˆì¸¡ ë¶„í¬ í™•ì¸
predicted_pIC50 = 9 - np.log10(submission_df['ASK1_IC50_nM'])
print(f"\nì˜ˆì¸¡ pIC50 ë¶„í¬:")
print(f"í‰ê· : {predicted_pIC50.mean():.2f}")
print(f"í‘œì¤€í¸ì°¨: {predicted_pIC50.std():.2f}")
print(f"ìµœì†Œê°’: {predicted_pIC50.min():.2f}")
print(f"ìµœëŒ€ê°’: {predicted_pIC50.max():.2f}")
print(f"pIC50 > 10 ì˜ˆì¸¡: {(predicted_pIC50 > 10).sum()}ê°œ")
print(f"pIC50 > 11 ì˜ˆì¸¡: {(predicted_pIC50 > 11).sum()}ê°œ")


# ì œì¶œ íŒŒì¼ ì €ì¥
submission_df.to_csv('./submission_catboost.csv', index=False)
print(f"\nì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: submission_catboost.csv")

# íŠ¹ì§• ì¤‘ìš”ë„ í™•ì¸
feature_importance = final_model.get_feature_importance()
feature_names = [f'embed_{i}' for i in range(train_embeddings.shape[1])] + feature_cols

# ìƒìœ„ 20ê°œ ì¤‘ìš” íŠ¹ì§•
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False).head(20)

print("\nìƒìœ„ 20ê°œ ì¤‘ìš” íŠ¹ì§•:")
print(importance_df)

# ëª¨ë¸ ì €ì¥
final_model.save_model('./best_catboost_model.cbm')
print("\nCatBoost ëª¨ë¸ ì €ì¥ ì™„ë£Œ: best_catboost_model.cbm")